{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "Bi09D6blan9Z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load the Model"
      ],
      "metadata": {
        "id": "dFliiJD-Zk7i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ5UoGfHZjEU",
        "outputId": "9491ff96-99d5-4447-872f-13ed425d3d1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 . Factual Prompt"
      ],
      "metadata": {
        "id": "oySeB1PEaY5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The factual prompt\n",
        "clean_prompt = \"The Space Needle is located in the city of\"\n",
        "\n",
        "# Tokenize the input prompt\n",
        "inputs = tokenizer(clean_prompt, return_tensors=\"pt\")\n",
        "# inputs"
      ],
      "metadata": {
        "id": "PENIbC3PZxON"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Token IDs\n",
        "# input_ids = inputs['input_ids'][0]  # Get the token IDs\n",
        "\n",
        "# # Convert token IDs to actual tokens\n",
        "# tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "# # Print the tokens\n",
        "# print(tokens)"
      ],
      "metadata": {
        "id": "rWQmLEyZg6Wj"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass through the model to get the outputs\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# # Print the logits (output predictions)\n",
        "# print(outputs.logits)"
      ],
      "metadata": {
        "id": "5_uH3nKKab1P"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to hook and capture only the hidden states (first element of the output tuple)\n",
        "hidden_states_clean = []\n",
        "\n",
        "# Hook function to capture clean hidden states\n",
        "def hook_fn_clean(module, input, output):\n",
        "    hidden_states_clean.append(output[0])\n",
        "\n",
        "# Register hooks to capture hidden states for each layer\n",
        "hooks_clean = []\n",
        "for i in range(model.config.n_layer):\n",
        "    hooks_clean.append(model.transformer.h[i].register_forward_hook(hook_fn_clean))\n",
        "\n",
        "# Run the clean model pass\n",
        "with torch.no_grad():\n",
        "    outputs_clean = model(**inputs)\n",
        "\n",
        "# Remove hooks after the clean run\n",
        "for hook in hooks_clean:\n",
        "    hook.remove()\n",
        "\n",
        "# Now hidden_states contains activations for all layers\n",
        "print(f\"Number of layers: {len(hidden_states_clean)}\")\n",
        "print(f\"Shape of hidden states from layer 1: {hidden_states_clean[0].shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QknNjhB8alqT",
        "outputId": "84a94a97-57ff-4934-abc3-a254f6e0243d"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers: 48\n",
            "Shape of hidden states from layer 1: torch.Size([1, 10, 1600])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set pad_token as eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Get the input IDs and attention mask for the clean prompt\n",
        "inputs_with_attention = tokenizer(clean_prompt, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# Generate output for the clean run with attention mask\n",
        "generated_outputs_clean = model.generate(\n",
        "    inputs_with_attention.input_ids,\n",
        "    attention_mask=inputs_with_attention.attention_mask,\n",
        "    max_length=11,\n",
        "    num_beams=1,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True,\n",
        "    pad_token_id=tokenizer.eos_token_id  # Explicitly set the pad token to eos token\n",
        ")\n",
        "\n",
        "# Decode the generated output\n",
        "clean_text = tokenizer.decode(generated_outputs_clean[0], skip_special_tokens=True)\n",
        "print(f\"Clean prediction: {clean_text.split()[-1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kMarO38zFBT",
        "outputId": "65a8f82d-91a2-41e0-ef89-d5415b5fdb99"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:615: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean prediction: Seattle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 . Corrupted Prompt"
      ],
      "metadata": {
        "id": "04ojFb7mhxnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **Controlled corruption**: Replace \"Space Needle\" with \"Eiffel Tower\"\n",
        "corrupted_prompt = \"The Eiffel Tower is located in the city of\"\n",
        "\n",
        "# Tokenize the corrupted prompt\n",
        "corrupted_inputs = tokenizer(corrupted_prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Initialize list to store hidden states from the corrupted run\n",
        "hidden_states_corrupted = []\n",
        "\n",
        "# Hook function to capture corrupted hidden states\n",
        "def hook_fn_corrupted(module, input, output):\n",
        "    hidden_states_corrupted.append(output[0])\n",
        "\n",
        "# Register hooks to capture hidden states for each layer during the corrupted run\n",
        "hooks_corrupted = []\n",
        "for i in range(model.config.n_layer):\n",
        "    hooks_corrupted.append(model.transformer.h[i].register_forward_hook(hook_fn_corrupted))\n",
        "\n",
        "# Run the corrupted model pass and collect activations\n",
        "with torch.no_grad():\n",
        "    corrupted_outputs = model(**corrupted_inputs)\n",
        "\n",
        "# Remove hooks after the corrupted run\n",
        "for hook in hooks_corrupted:\n",
        "    hook.remove()\n"
      ],
      "metadata": {
        "id": "j49t5W6ViBzo"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set pad_token as eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Get the input IDs and attention mask for the corrupt prompt\n",
        "inputs_with_attention = tokenizer(corrupted_prompt, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# Generate output for the corrupt run with attention mask\n",
        "generated_outputs_corrupted = model.generate(\n",
        "    inputs_with_attention.input_ids,\n",
        "    attention_mask=inputs_with_attention.attention_mask,\n",
        "    max_length=12,\n",
        "    num_beams=1,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode the generated output\n",
        "corrupt_text = tokenizer.decode(generated_outputs_corrupted[0], skip_special_tokens=True)\n",
        "print(f\"Corrupted prediction: {corrupt_text.split()[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnC2LCrv0BPt",
        "outputId": "925fd350-a6cc-4ecd-8d39-905b248f417a"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrupted prediction: Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 . Restoration"
      ],
      "metadata": {
        "id": "tSXCyphminsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_input = tokenizer.decode(inputs_with_attention.input_ids[0], skip_special_tokens=False)\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(inputs_with_attention.input_ids[0])\n",
        "\n",
        "# Print the tokenized input for reference\n",
        "print(f\"Decoded tokenized input: {decoded_tokens}\")\n",
        "print(f\"The subject: {decoded_tokens[1:4]}\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD4EZD0Gcibz",
        "outputId": "ff43448b-6cfe-47fe-ba12-4b8de6f3b86e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded tokenized input: ['The', 'ĠE', 'iff', 'el', 'ĠTower', 'Ġis', 'Ġlocated', 'Ġin', 'Ġthe', 'Ġcity', 'Ġof']\n",
            "The subject: ['ĠE', 'iff', 'el']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose layers to restore hidden states from\n",
        "layers_to_restore = range(0,48)\n",
        "\n",
        "# Tokenize the corrupted prompt to get the number of tokens\n",
        "num_tokens = inputs_with_attention.input_ids.shape[1]  # Get the number of tokens in the input\n",
        "\n",
        "\n",
        "# Loop over each layer\n",
        "for layer in layers_to_restore:  # Iterate over the selected layers\n",
        "    print(f\"Restoring hidden states for layer {layer} :\")\n",
        "\n",
        "    # Hook function to restore hidden states for all tokens except the last\n",
        "    def hook_fn_restoration(module, input, output):\n",
        "        restored_output = output[0].clone()\n",
        "\n",
        "        # Restore the clean hidden states for all tokens except the last one\n",
        "        for token_idx in range(1,4):\n",
        "            clean_state = hidden_states_clean[layer][0, token_idx, :]  # Get the clean hidden state for each token\n",
        "            restored_output[0, token_idx, :] = clean_state  # Restore clean state for each token\n",
        "\n",
        "        return (restored_output, *output[1:])\n",
        "\n",
        "    # Register the hook to restore clean activations at the specific layer for selected tokens\n",
        "    hooks_restoration = []\n",
        "    hooks_restoration.append(model.transformer.h[layer].register_forward_hook(hook_fn_restoration))\n",
        "\n",
        "    # Run the corrupted model pass with the restoration active\n",
        "    with torch.no_grad():\n",
        "        # Generate the output for the restored model while the hook is active\n",
        "        generated_outputs_restored = model.generate(\n",
        "            inputs_with_attention.input_ids,\n",
        "            attention_mask=inputs_with_attention.attention_mask,\n",
        "            max_length=12,\n",
        "            num_beams=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            early_stopping=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Remove the hooks after generating the output\n",
        "    for hook in hooks_restoration:\n",
        "        hook.remove()\n",
        "\n",
        "    # Decode the generated output\n",
        "    restored_text = tokenizer.decode(generated_outputs_restored[0], skip_special_tokens=True)\n",
        "    print(f\"Restored prediction for layer {layer}: {restored_text.split()[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YN2kgc0GMRi",
        "outputId": "315a4634-e3ff-41bb-ef5c-1c3ea22435e6"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restoring hidden states for layer 0 :\n",
            "Restored prediction for layer 0: Seattle\n",
            "Restoring hidden states for layer 1 :\n",
            "Restored prediction for layer 1: Seattle\n",
            "Restoring hidden states for layer 2 :\n",
            "Restored prediction for layer 2: Seattle\n",
            "Restoring hidden states for layer 3 :\n",
            "Restored prediction for layer 3: Seattle\n",
            "Restoring hidden states for layer 4 :\n",
            "Restored prediction for layer 4: Seattle\n",
            "Restoring hidden states for layer 5 :\n",
            "Restored prediction for layer 5: Seattle\n",
            "Restoring hidden states for layer 6 :\n",
            "Restored prediction for layer 6: Seattle\n",
            "Restoring hidden states for layer 7 :\n",
            "Restored prediction for layer 7: Seattle\n",
            "Restoring hidden states for layer 8 :\n",
            "Restored prediction for layer 8: Seattle\n",
            "Restoring hidden states for layer 9 :\n",
            "Restored prediction for layer 9: Seattle\n",
            "Restoring hidden states for layer 10 :\n",
            "Restored prediction for layer 10: Seattle\n",
            "Restoring hidden states for layer 11 :\n",
            "Restored prediction for layer 11: Seattle\n",
            "Restoring hidden states for layer 12 :\n",
            "Restored prediction for layer 12: Seattle\n",
            "Restoring hidden states for layer 13 :\n",
            "Restored prediction for layer 13: Seattle\n",
            "Restoring hidden states for layer 14 :\n",
            "Restored prediction for layer 14: Seattle\n",
            "Restoring hidden states for layer 15 :\n",
            "Restored prediction for layer 15: Seattle\n",
            "Restoring hidden states for layer 16 :\n",
            "Restored prediction for layer 16: Seattle\n",
            "Restoring hidden states for layer 17 :\n",
            "Restored prediction for layer 17: Seattle\n",
            "Restoring hidden states for layer 18 :\n",
            "Restored prediction for layer 18: Seattle\n",
            "Restoring hidden states for layer 19 :\n",
            "Restored prediction for layer 19: Seattle\n",
            "Restoring hidden states for layer 20 :\n",
            "Restored prediction for layer 20: Seattle\n",
            "Restoring hidden states for layer 21 :\n",
            "Restored prediction for layer 21: Seattle\n",
            "Restoring hidden states for layer 22 :\n",
            "Restored prediction for layer 22: Seattle\n",
            "Restoring hidden states for layer 23 :\n",
            "Restored prediction for layer 23: Seattle\n",
            "Restoring hidden states for layer 24 :\n",
            "Restored prediction for layer 24: Seattle\n",
            "Restoring hidden states for layer 25 :\n",
            "Restored prediction for layer 25: Seattle\n",
            "Restoring hidden states for layer 26 :\n",
            "Restored prediction for layer 26: Seattle\n",
            "Restoring hidden states for layer 27 :\n",
            "Restored prediction for layer 27: Seattle\n",
            "Restoring hidden states for layer 28 :\n",
            "Restored prediction for layer 28: Seattle\n",
            "Restoring hidden states for layer 29 :\n",
            "Restored prediction for layer 29: Seattle\n",
            "Restoring hidden states for layer 30 :\n",
            "Restored prediction for layer 30: Paris\n",
            "Restoring hidden states for layer 31 :\n",
            "Restored prediction for layer 31: Paris\n",
            "Restoring hidden states for layer 32 :\n",
            "Restored prediction for layer 32: Paris\n",
            "Restoring hidden states for layer 33 :\n",
            "Restored prediction for layer 33: Paris\n",
            "Restoring hidden states for layer 34 :\n",
            "Restored prediction for layer 34: Paris\n",
            "Restoring hidden states for layer 35 :\n",
            "Restored prediction for layer 35: Paris\n",
            "Restoring hidden states for layer 36 :\n",
            "Restored prediction for layer 36: Paris\n",
            "Restoring hidden states for layer 37 :\n",
            "Restored prediction for layer 37: Paris\n",
            "Restoring hidden states for layer 38 :\n",
            "Restored prediction for layer 38: Paris\n",
            "Restoring hidden states for layer 39 :\n",
            "Restored prediction for layer 39: Paris\n",
            "Restoring hidden states for layer 40 :\n",
            "Restored prediction for layer 40: Paris\n",
            "Restoring hidden states for layer 41 :\n",
            "Restored prediction for layer 41: Paris\n",
            "Restoring hidden states for layer 42 :\n",
            "Restored prediction for layer 42: Paris\n",
            "Restoring hidden states for layer 43 :\n",
            "Restored prediction for layer 43: Paris\n",
            "Restoring hidden states for layer 44 :\n",
            "Restored prediction for layer 44: Paris\n",
            "Restoring hidden states for layer 45 :\n",
            "Restored prediction for layer 45: Paris\n",
            "Restoring hidden states for layer 46 :\n",
            "Restored prediction for layer 46: Paris\n",
            "Restoring hidden states for layer 47 :\n",
            "Restored prediction for layer 47: Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ncbpHmDHhbb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}