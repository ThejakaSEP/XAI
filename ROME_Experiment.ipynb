{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "Bi09D6blan9Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load the Model"
      ],
      "metadata": {
        "id": "dFliiJD-Zk7i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "jQ5UoGfHZjEU"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 . Factual Prompt"
      ],
      "metadata": {
        "id": "oySeB1PEaY5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The factual prompt\n",
        "prompt = \"The Space Needle is located in the city of\"\n",
        "\n",
        "# Tokenize the input prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "# inputs"
      ],
      "metadata": {
        "id": "PENIbC3PZxON"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Token IDs\n",
        "# input_ids = inputs['input_ids'][0]  # Get the token IDs\n",
        "\n",
        "# # Convert token IDs to actual tokens\n",
        "# tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "# # Print the tokens\n",
        "# print(tokens)"
      ],
      "metadata": {
        "id": "rWQmLEyZg6Wj"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass through the model to get the outputs\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# # Print the logits (output predictions)\n",
        "# print(outputs.logits)"
      ],
      "metadata": {
        "id": "5_uH3nKKab1P"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to hook and capture only the hidden states (first element of the output tuple)\n",
        "hidden_states_clean = []\n",
        "\n",
        "# Hook function to capture clean hidden states\n",
        "def hook_fn_clean(module, input, output):\n",
        "    hidden_states_clean.append(output[0])\n",
        "\n",
        "# Register hooks to capture hidden states for each layer\n",
        "hooks_clean = []\n",
        "for i in range(model.config.n_layer):\n",
        "    hooks_clean.append(model.transformer.h[i].register_forward_hook(hook_fn_clean))\n",
        "\n",
        "# Run the clean model pass\n",
        "with torch.no_grad():\n",
        "    outputs_clean = model(**inputs)\n",
        "\n",
        "# Remove hooks after the clean run\n",
        "for hook in hooks_clean:\n",
        "    hook.remove()\n",
        "\n",
        "# Now hidden_states contains activations for all layers\n",
        "print(f\"Number of layers: {len(hidden_states_clean)}\")\n",
        "print(f\"Shape of hidden states from layer 1: {hidden_states_clean[0].shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QknNjhB8alqT",
        "outputId": "4dd6f6ae-dc15-4938-9a7f-f450deeabbda"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers: 48\n",
            "Shape of hidden states from layer 1: torch.Size([1, 10, 1600])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set pad_token as eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Get the input IDs and attention mask for the clean prompt\n",
        "inputs_with_attention = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# Generate output for the clean run with attention mask\n",
        "generated_outputs_clean = model.generate(\n",
        "    inputs_with_attention.input_ids,\n",
        "    attention_mask=inputs_with_attention.attention_mask,\n",
        "    max_length=11,\n",
        "    num_beams=1,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True,\n",
        "    pad_token_id=tokenizer.eos_token_id  # Explicitly set the pad token to eos token\n",
        ")\n",
        "\n",
        "# Decode the generated output\n",
        "clean_text = tokenizer.decode(generated_outputs_clean[0], skip_special_tokens=True)\n",
        "print(f\"Clean prediction: {clean_text.split()[-1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kMarO38zFBT",
        "outputId": "f6dd1f8d-dca9-416d-ea51-37239646635d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean prediction: Seattle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 . Corrupted Prompt"
      ],
      "metadata": {
        "id": "04ojFb7mhxnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **Controlled corruption**: Replace \"Space Needle\" with \"Eiffel Tower\"\n",
        "corrupted_prompt = \"The Eiffel Tower is located in the city of\"\n",
        "\n",
        "# Tokenize the corrupted prompt\n",
        "corrupted_inputs = tokenizer(corrupted_prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Initialize list to store hidden states from the corrupted run\n",
        "hidden_states_corrupted = []\n",
        "\n",
        "# Hook function to capture corrupted hidden states\n",
        "def hook_fn_corrupted(module, input, output):\n",
        "    hidden_states_corrupted.append(output[0])\n",
        "\n",
        "# Register hooks to capture hidden states for each layer during the corrupted run\n",
        "hooks_corrupted = []\n",
        "for i in range(model.config.n_layer):\n",
        "    hooks_corrupted.append(model.transformer.h[i].register_forward_hook(hook_fn_corrupted))\n",
        "\n",
        "# Run the corrupted model pass and collect activations\n",
        "with torch.no_grad():\n",
        "    corrupted_outputs = model(**corrupted_inputs)\n",
        "\n",
        "# Remove hooks after the corrupted run\n",
        "for hook in hooks_corrupted:\n",
        "    hook.remove()\n"
      ],
      "metadata": {
        "id": "j49t5W6ViBzo"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set pad_token as eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Get the input IDs and attention mask for the corrupt prompt\n",
        "inputs_with_attention = tokenizer(corrupted_prompt, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# Generate output for the corrupt run with attention mask\n",
        "generated_outputs_corrupted = model.generate(\n",
        "    inputs_with_attention.input_ids,\n",
        "    attention_mask=inputs_with_attention.attention_mask,\n",
        "    max_length=12,\n",
        "    num_beams=1,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode the generated output\n",
        "corrupt_text = tokenizer.decode(generated_outputs_corrupted[0], skip_special_tokens=True)\n",
        "print(f\"Clean prediction: {corrupt_text.split()[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnC2LCrv0BPt",
        "outputId": "9b5c3436-d3ec-41ce-cd77-4c2d6bc6b5f2"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean prediction: Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 . Restoration"
      ],
      "metadata": {
        "id": "tSXCyphminsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decide which layer and token to restore\n",
        "layer_to_restore = 12  # Example: restoring layer 15\n",
        "token_to_restore = 1  # Example: token index of subject (e.g., \"Eiffel Tower\" in corrupted prompt)\n",
        "\n",
        "# Hook function for corrupted-with-restoration\n",
        "hidden_states_restored = []\n",
        "\n",
        "def hook_fn_restoration(module, input, output):\n",
        "    # Replace the corrupted hidden state with the clean hidden state at the specified layer and token\n",
        "    clean_state = hidden_states_clean[layer_to_restore][0, token_to_restore, :]\n",
        "    restored_output = output[0].clone()\n",
        "    restored_output[0, token_to_restore, :] = clean_state  # Restore the clean state at this layer and token\n",
        "    hidden_states_restored.append(restored_output)\n",
        "    return (restored_output, *output[1:])\n",
        "\n",
        "# Register the hook to restore clean activations at the specific layer and token\n",
        "hooks_restoration = []\n",
        "hooks_restoration.append(model.transformer.h[layer_to_restore].register_forward_hook(hook_fn_restoration))\n",
        "\n",
        "# Run the corrupted model pass again with the restoration\n",
        "with torch.no_grad():\n",
        "    restored_outputs = model(**corrupted_inputs)\n",
        "\n",
        "# Remove the hooks after restoration\n",
        "for hook in hooks_restoration:\n",
        "    hook.remove()\n",
        "\n",
        "\n",
        "# Set pad_token as eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Get the input IDs and attention mask for the corrupt prompt\n",
        "inputs_with_attention = tokenizer(corrupted_prompt, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# Generate output for the corrupt run with attention mask\n",
        "generated_outputs_restored = model.generate(\n",
        "    inputs_with_attention.input_ids,\n",
        "    attention_mask=inputs_with_attention.attention_mask,\n",
        "    max_length=12,\n",
        "    num_beams=1,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode the generated output\n",
        "restored_text = tokenizer.decode(generated_outputs_restored[0], skip_special_tokens=True)\n",
        "print(f\"Clean prediction: {restored_text.split()[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKkKcdtAiqqW",
        "outputId": "f1f8b141-86c6-4a80-c1e9-7ac875fa1fdc"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean prediction: Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set pad_token as eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Get the input IDs and attention mask for the corrupt prompt\n",
        "inputs_with_attention = tokenizer(corrupted_prompt, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# Generate output for the corrupt run with attention mask\n",
        "generated_outputs_restored = model.generate(\n",
        "    inputs_with_attention.input_ids,\n",
        "    attention_mask=inputs_with_attention.attention_mask,\n",
        "    max_length=12,\n",
        "    num_beams=1,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode the generated output\n",
        "restored_text = tokenizer.decode(generated_outputs_restored[0], skip_special_tokens=True)\n",
        "print(f\"Clean prediction: {restored_text.split()[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqaOko6KuUl-",
        "outputId": "b407874e-4994-4da8-c9c6-882dcded886d"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean prediction: Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y9h_FJ7u2a7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loop"
      ],
      "metadata": {
        "id": "WFB8z5Am4aRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try different layers and token indices for restoration\n",
        "for layer in range(10, 20):  # Experiment with layers between 10 and 20\n",
        "    for token_idx in [1, 2]:  # Token indices for 'Space' and 'Needle'\n",
        "        print(f\"Testing layer: {layer}, token index: {token_idx}\")\n",
        "\n",
        "        # Hook function for restoration\n",
        "        def hook_fn_restoration(module, input, output):\n",
        "            clean_state = hidden_states_clean[layer][0, token_idx, :]\n",
        "            restored_output = output[0].clone()\n",
        "            restored_output[0, token_idx, :] = clean_state\n",
        "            return (restored_output, *output[1:])\n",
        "\n",
        "        # Register the hook\n",
        "        hooks_restoration = []\n",
        "        hooks_restoration.append(model.transformer.h[layer].register_forward_hook(hook_fn_restoration))\n",
        "\n",
        "        # Run the corrupted model pass again with the restoration\n",
        "        with torch.no_grad():\n",
        "            restored_outputs = model(**corrupted_inputs)\n",
        "\n",
        "        # Remove the hooks after restoration\n",
        "        for hook in hooks_restoration:\n",
        "            hook.remove()\n",
        "\n",
        "        # Generate the output for the restored model\n",
        "        generated_outputs_restored = model.generate(\n",
        "            inputs_with_attention.input_ids,\n",
        "            attention_mask=inputs_with_attention.attention_mask,\n",
        "            max_length=12,\n",
        "            num_beams=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            early_stopping=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Decode the generated output\n",
        "        restored_text = tokenizer.decode(generated_outputs_restored[0], skip_special_tokens=True)\n",
        "        print(f\"Restored prediction: {restored_text.split()[-1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojvg8Clk4bR6",
        "outputId": "f9cf0f96-9a17-476b-d294-1529436c18c9"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing layer: 10, token index: 1\n",
            "Restored prediction: Paris\n",
            "Testing layer: 10, token index: 2\n",
            "Restored prediction: Paris\n",
            "Testing layer: 11, token index: 1\n",
            "Restored prediction: Paris\n",
            "Testing layer: 11, token index: 2\n",
            "Restored prediction: Paris\n",
            "Testing layer: 12, token index: 1\n",
            "Restored prediction: Paris\n",
            "Testing layer: 12, token index: 2\n",
            "Restored prediction: Paris\n",
            "Testing layer: 13, token index: 1\n",
            "Restored prediction: Paris\n",
            "Testing layer: 13, token index: 2\n",
            "Restored prediction: Paris\n",
            "Testing layer: 14, token index: 1\n",
            "Restored prediction: Paris\n",
            "Testing layer: 14, token index: 2\n",
            "Restored prediction: Paris\n",
            "Testing layer: 15, token index: 1\n",
            "Restored prediction: Paris\n",
            "Testing layer: 15, token index: 2\n",
            "Restored prediction: Paris\n",
            "Testing layer: 16, token index: 1\n",
            "Restored prediction: Paris\n",
            "Testing layer: 16, token index: 2\n",
            "Restored prediction: Paris\n",
            "Testing layer: 17, token index: 1\n",
            "Restored prediction: Paris\n",
            "Testing layer: 17, token index: 2\n",
            "Restored prediction: Paris\n",
            "Testing layer: 18, token index: 1\n",
            "Restored prediction: Paris\n",
            "Testing layer: 18, token index: 2\n",
            "Restored prediction: Paris\n",
            "Testing layer: 19, token index: 1\n",
            "Restored prediction: Paris\n",
            "Testing layer: 19, token index: 2\n",
            "Restored prediction: Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yRIlCru94blM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}