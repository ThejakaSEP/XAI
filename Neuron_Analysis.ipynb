{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1773,"status":"ok","timestamp":1726969920647,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"},"user_tz":240},"id":"R4kVMrRmXSaK","outputId":"0a878140-3825-45af-fbc8-f0d564dc2c8e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cpu)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}],"source":["!pip install torch transformers numpy"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":786,"status":"ok","timestamp":1726971473809,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"},"user_tz":240},"id":"DhrOZVhqX5V8","outputId":"2fa4b01e-e527-47d8-8d4a-82cbd7d9cb7a"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","# Load the tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","\n","# Set model in evaluation mode\n","model.eval()\n","\n","# Example input text\n","input_text = \"What is the capital of France? A) London B) Paris C) Berlin D) Rome\"\n","\n","# Tokenize input\n","input_ids = tokenizer.encode(input_text, return_tensors='pt')\n"]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":311,"status":"ok","timestamp":1726971474611,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"},"user_tz":240},"id":"1eE0YZ_1YEOw"},"outputs":[],"source":["# Get model output (logits)\n","with torch.no_grad():\n","    outputs = model(input_ids)\n","logits = outputs.logits\n","\n","# Get the logits for the last token (this is the \"answer\")\n","final_logits = logits[:, -1, :]"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155,"status":"ok","timestamp":1726971475433,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"},"user_tz":240},"id":"pIWK7qwSYLwJ","outputId":"f634c8d4-4af2-492e-cab1-2416346cc231"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-78.3749, -80.8012, -83.3722,  ..., -94.4622, -91.0560, -77.0115]])"]},"metadata":{},"execution_count":54}],"source":["final_logits"]},{"cell_type":"code","source":["# list(model.named_modules())"],"metadata":{"id":"Wli0JjI8ejxL","executionInfo":{"status":"ok","timestamp":1726971476078,"user_tz":240,"elapsed":150,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","execution_count":57,"metadata":{"executionInfo":{"elapsed":128,"status":"ok","timestamp":1726971517060,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"},"user_tz":240},"id":"lfi79n3WYPhn"},"outputs":[],"source":["activations = {}\n","\n","# hook function to accept the layer/module name\n","def create_hook_fn(name):\n","    def hook_fn(module, input, output):\n","        # Store the activations using the passed 'name'\n","        activations[name] = output\n","    return hook_fn\n","\n","# Register hooks for attention and MLP layers with the layer name\n","for name, module in model.named_modules():\n","    if 'attn' in name or 'mlp' in name:\n","        module.register_forward_hook(create_hook_fn(name))\n","\n","# Run the forward pass again with hooks activated\n","with torch.no_grad():\n","    outputs = model(input_ids)"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":151,"status":"ok","timestamp":1726971520308,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"},"user_tz":240},"id":"aOZ1SzQlYYRW","outputId":"7462c46b-7314-4fa5-fedd-5e1259d40dcc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["240"]},"metadata":{},"execution_count":58}],"source":["len(activations)"]},{"cell_type":"code","source":["# # Print the full model structure to identify attention and MLP layers\n","# for name, module in model.named_modules():\n","#     print(name)"],"metadata":{"id":"4mIGCjaAcnrH","executionInfo":{"status":"ok","timestamp":1726971520652,"user_tz":240,"elapsed":3,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["# activations"],"metadata":{"id":"pq3LNKW2hkQZ","executionInfo":{"status":"ok","timestamp":1726971541617,"user_tz":240,"elapsed":131,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["# print(activations.keys())"],"metadata":{"id":"30R2FTKydjJC","executionInfo":{"status":"ok","timestamp":1726971548354,"user_tz":240,"elapsed":149,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","execution_count":66,"metadata":{"executionInfo":{"elapsed":126,"status":"ok","timestamp":1726971558918,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"},"user_tz":240},"id":"kedbFgXjYb1R"},"outputs":[],"source":["# Calculate direct contribution of a component\n","def compute_direct_contribution(model, input_ids, activations, component_name):\n","    # Backup the original activation\n","    original_activation = activations[component_name].clone()\n","\n","    # Set activation to zero\n","    activations[component_name].zero_()\n","\n","    # Forward pass with zeroed-out component\n","    with torch.no_grad():\n","        outputs = model(input_ids)\n","        logits_zeroed = outputs.logits[:, -1, :]\n","\n","    # Compute the difference in logits (direct effect)\n","    direct_effect = logits - logits_zeroed\n","\n","    # Restore the original activation\n","    activations[component_name] = original_activation\n","\n","    return direct_effect\n"]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"elapsed":170,"status":"error","timestamp":1726971773670,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"},"user_tz":240},"id":"BeL8Bq3pbDO4","outputId":"f92a451a-1a1c-40a6-9ccb-024ce0ec80e1"},"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'transformer.h.0.attn'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-70-e4fe1dadaafc>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Compute patched logits for each component\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mpatched_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_patching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transformer.h.0.attn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-70-e4fe1dadaafc>\u001b[0m in \u001b[0;36mactivation_patching\u001b[0;34m(model, input_ids, activations, patch_activations, component_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Replace activation with the one from a different prompt (also handle tuple case)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mpatched_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_activations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcomponent_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatched_activation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mpatched_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatched_activation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Extract the tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0minner_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'transformer.h.0.attn'"]}],"source":["# New input for patching\n","patch_input_text = \"What is the capital of Germany? A) London B) Berlin C) Paris D) Rome\"\n","patch_input_ids = tokenizer.encode(patch_input_text, return_tensors='pt')\n","\n","# Get patch activations\n","with torch.no_grad():\n","    patch_outputs = model(patch_input_ids)\n","\n","def activation_patching(model, input_ids, activations, patch_activations, component_name):\n","    # Check if activation is a tuple and get the first element\n","    original_activation = activations[component_name]\n","    if isinstance(original_activation, tuple):\n","        original_activation = original_activation[0]  # Extract the first element (tensor)\n","\n","    # Clone the activation\n","    original_activation = original_activation.clone()\n","\n","    # Replace activation with the one from a different prompt (also handle tuple case)\n","    patched_activation = patch_activations[component_name]\n","    if isinstance(patched_activation, tuple):\n","        patched_activation = patched_activation[0]  # Extract the tensor\n","\n","    # Patch the activation\n","    activations[component_name] = patched_activation.clone()\n","\n","    # Run the forward pass with patched activations\n","    with torch.no_grad():\n","        outputs = model(input_ids)\n","        patched_logits = outputs.logits[:, -1, :]\n","\n","    # Restore original activation\n","    activations[component_name] = original_activation\n","\n","    return patched_logits\n","\n","\n","# Compute patched logits for each component\n","patched_logits = activation_patching(model, input_ids, activations, patch_outputs, 'transformer.h.0.attn')\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"QV06dX7ubFF9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726969979607,"user_tz":240,"elapsed":146,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}},"outputId":"c283ad0a-a08d-4579-9044-6f9428679146"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","transformer\n","transformer.wte\n","transformer.wpe\n","transformer.drop\n","transformer.h\n","transformer.h.0\n","transformer.h.0.ln_1\n","transformer.h.0.attn\n","transformer.h.0.attn.c_attn\n","transformer.h.0.attn.c_proj\n","transformer.h.0.attn.attn_dropout\n","transformer.h.0.attn.resid_dropout\n","transformer.h.0.ln_2\n","transformer.h.0.mlp\n","transformer.h.0.mlp.c_fc\n","transformer.h.0.mlp.c_proj\n","transformer.h.0.mlp.act\n","transformer.h.0.mlp.dropout\n","transformer.h.1\n","transformer.h.1.ln_1\n","transformer.h.1.attn\n","transformer.h.1.attn.c_attn\n","transformer.h.1.attn.c_proj\n","transformer.h.1.attn.attn_dropout\n","transformer.h.1.attn.resid_dropout\n","transformer.h.1.ln_2\n","transformer.h.1.mlp\n","transformer.h.1.mlp.c_fc\n","transformer.h.1.mlp.c_proj\n","transformer.h.1.mlp.act\n","transformer.h.1.mlp.dropout\n","transformer.h.2\n","transformer.h.2.ln_1\n","transformer.h.2.attn\n","transformer.h.2.attn.c_attn\n","transformer.h.2.attn.c_proj\n","transformer.h.2.attn.attn_dropout\n","transformer.h.2.attn.resid_dropout\n","transformer.h.2.ln_2\n","transformer.h.2.mlp\n","transformer.h.2.mlp.c_fc\n","transformer.h.2.mlp.c_proj\n","transformer.h.2.mlp.act\n","transformer.h.2.mlp.dropout\n","transformer.h.3\n","transformer.h.3.ln_1\n","transformer.h.3.attn\n","transformer.h.3.attn.c_attn\n","transformer.h.3.attn.c_proj\n","transformer.h.3.attn.attn_dropout\n","transformer.h.3.attn.resid_dropout\n","transformer.h.3.ln_2\n","transformer.h.3.mlp\n","transformer.h.3.mlp.c_fc\n","transformer.h.3.mlp.c_proj\n","transformer.h.3.mlp.act\n","transformer.h.3.mlp.dropout\n","transformer.h.4\n","transformer.h.4.ln_1\n","transformer.h.4.attn\n","transformer.h.4.attn.c_attn\n","transformer.h.4.attn.c_proj\n","transformer.h.4.attn.attn_dropout\n","transformer.h.4.attn.resid_dropout\n","transformer.h.4.ln_2\n","transformer.h.4.mlp\n","transformer.h.4.mlp.c_fc\n","transformer.h.4.mlp.c_proj\n","transformer.h.4.mlp.act\n","transformer.h.4.mlp.dropout\n","transformer.h.5\n","transformer.h.5.ln_1\n","transformer.h.5.attn\n","transformer.h.5.attn.c_attn\n","transformer.h.5.attn.c_proj\n","transformer.h.5.attn.attn_dropout\n","transformer.h.5.attn.resid_dropout\n","transformer.h.5.ln_2\n","transformer.h.5.mlp\n","transformer.h.5.mlp.c_fc\n","transformer.h.5.mlp.c_proj\n","transformer.h.5.mlp.act\n","transformer.h.5.mlp.dropout\n","transformer.h.6\n","transformer.h.6.ln_1\n","transformer.h.6.attn\n","transformer.h.6.attn.c_attn\n","transformer.h.6.attn.c_proj\n","transformer.h.6.attn.attn_dropout\n","transformer.h.6.attn.resid_dropout\n","transformer.h.6.ln_2\n","transformer.h.6.mlp\n","transformer.h.6.mlp.c_fc\n","transformer.h.6.mlp.c_proj\n","transformer.h.6.mlp.act\n","transformer.h.6.mlp.dropout\n","transformer.h.7\n","transformer.h.7.ln_1\n","transformer.h.7.attn\n","transformer.h.7.attn.c_attn\n","transformer.h.7.attn.c_proj\n","transformer.h.7.attn.attn_dropout\n","transformer.h.7.attn.resid_dropout\n","transformer.h.7.ln_2\n","transformer.h.7.mlp\n","transformer.h.7.mlp.c_fc\n","transformer.h.7.mlp.c_proj\n","transformer.h.7.mlp.act\n","transformer.h.7.mlp.dropout\n","transformer.h.8\n","transformer.h.8.ln_1\n","transformer.h.8.attn\n","transformer.h.8.attn.c_attn\n","transformer.h.8.attn.c_proj\n","transformer.h.8.attn.attn_dropout\n","transformer.h.8.attn.resid_dropout\n","transformer.h.8.ln_2\n","transformer.h.8.mlp\n","transformer.h.8.mlp.c_fc\n","transformer.h.8.mlp.c_proj\n","transformer.h.8.mlp.act\n","transformer.h.8.mlp.dropout\n","transformer.h.9\n","transformer.h.9.ln_1\n","transformer.h.9.attn\n","transformer.h.9.attn.c_attn\n","transformer.h.9.attn.c_proj\n","transformer.h.9.attn.attn_dropout\n","transformer.h.9.attn.resid_dropout\n","transformer.h.9.ln_2\n","transformer.h.9.mlp\n","transformer.h.9.mlp.c_fc\n","transformer.h.9.mlp.c_proj\n","transformer.h.9.mlp.act\n","transformer.h.9.mlp.dropout\n","transformer.h.10\n","transformer.h.10.ln_1\n","transformer.h.10.attn\n","transformer.h.10.attn.c_attn\n","transformer.h.10.attn.c_proj\n","transformer.h.10.attn.attn_dropout\n","transformer.h.10.attn.resid_dropout\n","transformer.h.10.ln_2\n","transformer.h.10.mlp\n","transformer.h.10.mlp.c_fc\n","transformer.h.10.mlp.c_proj\n","transformer.h.10.mlp.act\n","transformer.h.10.mlp.dropout\n","transformer.h.11\n","transformer.h.11.ln_1\n","transformer.h.11.attn\n","transformer.h.11.attn.c_attn\n","transformer.h.11.attn.c_proj\n","transformer.h.11.attn.attn_dropout\n","transformer.h.11.attn.resid_dropout\n","transformer.h.11.ln_2\n","transformer.h.11.mlp\n","transformer.h.11.mlp.c_fc\n","transformer.h.11.mlp.c_proj\n","transformer.h.11.mlp.act\n","transformer.h.11.mlp.dropout\n","transformer.ln_f\n","lm_head\n"]}],"source":["# Print all named modules to find the correct name\n","for name, module in model.named_modules():\n","    print(name)"]},{"cell_type":"code","source":[],"metadata":{"id":"4t8GYR1ebqG9"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOtSU9/GKO/xjwBr7m2fv0L"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}