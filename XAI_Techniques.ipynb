{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "kVJf_DZxTc2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Dependencies"
      ],
      "metadata": {
        "id": "AKYMGxf5TgkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers bertviz shap captum"
      ],
      "metadata": {
        "id": "KU-fBf3wTnJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN'] = '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'"
      ],
      "metadata": {
        "id": "oU6Et9hUWKUG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers.generation import GenerationConfig\n",
        "from transformers import utils\n",
        "import shap\n",
        "from bertviz import model_view, head_view\n",
        "from captum.attr import(\n",
        "    FeatureAblation,\n",
        "    LLMAttribution,\n",
        "    TextTokenInput,\n",
        "    TextTemplateInput,\n",
        "    ProductBaselines,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy9x6iWHTnFJ",
        "outputId": "a8d3700c-70ba-4fa6-b579-40d651a5091b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 11.5 ms, sys: 114 Âµs, total: 11.6 ms\n",
            "Wall time: 888 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load LLM (LLAMA 2)"
      ],
      "metadata": {
        "id": "mfcKlh9jVUZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "access_token = \"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\"\n",
        "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, token=access_token)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model, token=access_token)\n"
      ],
      "metadata": {
        "id": "7MWUAetgXRrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "odryk2DtThFU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-HCQ2laTOc2"
      },
      "outputs": [],
      "source": []
    }
  ]
}